name: Daily Log Anomaly Analysis

on:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * *' 

permissions:
  models: read
  contents: read

env:
  LOG_FILE_PATH: "logs/nginx-combined.log"  # Combined nginx access and error logs
  ANALYSIS_PROMPT: |
    You are a nginx web server expert. Analyze this combined nginx log file that contains both access logs and error logs. 
    Please identify and categorize any anomalies, security incidents, performance issues, and operational problems. 
    Structure your response with clear sections for: 
    1) Executive Summary, 
    2) Security Incidents (attacks, unauthorized access, suspicious IPs), 
    3) System Performance Issues (slow responses, upstream problems, resource issues), 
    4) Operational Problems (errors, SSL issues, configuration problems), 
    5) Traffic Patterns (bot activity, user behavior, rate limiting), 
    6) Recommendations. 
    Here is the log content:

jobs:
  analyze-logs:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Analyze logs for anomalies
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ANALYSIS_PROMPT: ${{ env.ANALYSIS_PROMPT }}
        run: |
          # Use Python to properly handle log content and create JSON request
          python3 -c "
          import json
          import string
          import sys
          import os
          
          try:
              # Read the log content and handle any encoding issues
              with open('${{ env.LOG_FILE_PATH }}', 'r', encoding='utf-8', errors='replace') as f:
                  log_content = f.read()
              
              # Clean the log content to remove any problematic control characters
              # Keep only printable characters and common whitespace
              printable_chars = string.printable
              cleaned_content = ''.join(char if char in printable_chars else ' ' for char in log_content)
              
              # Get the analysis prompt from environment variable
              analysis_prompt = os.environ.get('ANALYSIS_PROMPT', 'Analyze this log file for anomalies.')
              
              # Create the API request
              request_data = {
                  'messages': [
                      {
                          'role': 'user',
                          'content': analysis_prompt + '\\n\\n' + cleaned_content
                      }
                  ],
                  'model': 'openai/gpt-4o'
              }
              
              # Write the properly formatted JSON request
              with open('/tmp/final_request.json', 'w') as f:
                  json.dump(request_data, f)
              
              print('API request prepared successfully')
          
          except Exception as e:
              print(f'Error preparing request: {e}')
              sys.exit(1)
          "
          
          # Call AI model to analyze logs
          RESPONSE=$(curl -s "https://models.github.ai/inference/chat/completions" \
             -H "Content-Type: application/json" \
             -H "Authorization: Bearer $GITHUB_TOKEN" \
             -d @/tmp/final_request.json)
          
          echo "=== 📊 NGINX COMBINED LOG ANALYSIS REPORT ==="
          echo "Generated on: $(date)"
          echo "Log file: ${{ env.LOG_FILE_PATH }}"
          echo "Analysis powered by: GitHub Models (GPT-4o)"
          echo "=================================="
          echo ""
          
          # Capture the analysis results
          ANALYSIS_OUTPUT=$(echo "$RESPONSE" | python3 -c "
          import sys, json
          try:
              data = json.load(sys.stdin)
              if 'choices' in data and len(data['choices']) > 0:
                  print(data['choices'][0]['message']['content'])
              else:
                  print('❌ Error: Unexpected response format')
          except Exception as e:
              print('❌ Error processing response:', str(e))
          ")
          
          # Display the results
          echo "$ANALYSIS_OUTPUT"
          
          # Create GitHub Job Summary
          echo "# 🌐 Nginx Combined Log Analysis Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Generated:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Repository:** ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
          echo "**Log File:** \`${{ env.LOG_FILE_PATH }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Analysis Model:** GitHub Models (GPT-4o)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## 📈 Log Statistics" >> $GITHUB_STEP_SUMMARY
          echo "- **Total lines:** $(wc -l < ${{ env.LOG_FILE_PATH }})" >> $GITHUB_STEP_SUMMARY
          echo "- **File size:** $(du -h ${{ env.LOG_FILE_PATH }} | cut -f1)" >> $GITHUB_STEP_SUMMARY
          echo "- **Access log entries:** $(grep -c '^[0-9]' ${{ env.LOG_FILE_PATH }} || echo '0')" >> $GITHUB_STEP_SUMMARY
          echo "- **Error log entries:** $(grep -c '^\[0-9\|^2025/' ${{ env.LOG_FILE_PATH }} || echo '0')" >> $GITHUB_STEP_SUMMARY
          echo "- **4xx responses:** $(grep -c ' 4[0-9][0-9] ' ${{ env.LOG_FILE_PATH }} || echo '0')" >> $GITHUB_STEP_SUMMARY
          echo "- **5xx responses:** $(grep -c ' 5[0-9][0-9] ' ${{ env.LOG_FILE_PATH }} || echo '0')" >> $GITHUB_STEP_SUMMARY
          echo "- **Error entries:** $(grep -c '\[error\]' ${{ env.LOG_FILE_PATH }} || echo '0')" >> $GITHUB_STEP_SUMMARY
          echo "- **Warning entries:** $(grep -c '\[warn\]' ${{ env.LOG_FILE_PATH }} || echo '0')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## 🔍 AI Analysis Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add the analysis results to the job summary
          echo "$ANALYSIS_OUTPUT" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*This analysis was automatically generated by GitHub Actions using GitHub Models API*" >> $GITHUB_STEP_SUMMARY