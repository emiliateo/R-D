name: Daily Log Anomaly Analysis

on:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * *' 

permissions:
  models: read
  contents: read

env:
  LOG_FILE_PATH: "logs/nginx.log"  # Change this path if your log file is in a different location
  ANALYSIS_PROMPT: |
    You are a log analyst. Analyze these logs and provide a SHORT, CLEAR report focusing on errors and problems.
    
    Keep your response CONCISE and use simple language. Focus only on:
    - Critical errors and failures
    - Security threats (attacks, suspicious activity)
    - System problems that need fixing
    
    Format your response in 3 SHORT sections:
    1) **ðŸš¨ CRITICAL ISSUES** (most important problems - max 3 bullet points)
    2) **âš ï¸ ERRORS FOUND** (list main error types - max 5 bullet points)  
    3) **ðŸ”§ QUICK FIXES** (what to do next - max 3 action items)
    
    Use bullet points and keep each point to 1-2 sentences. Skip technical jargon. Here are the logs:

jobs:
  analyze-logs:
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Analyze logs for anomalies
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ANALYSIS_PROMPT: ${{ env.ANALYSIS_PROMPT }}
        run: |
          # Use Python to properly handle log content and create JSON request
          python3 -c "
          import json
          import string
          import sys
          import os
          
          try:
              # Read the log content and handle any encoding issues
              with open('${{ env.LOG_FILE_PATH }}', 'r', encoding='utf-8', errors='replace') as f:
                  log_content = f.read()
              
              # Clean the log content to remove any problematic control characters
              # Keep only printable characters and common whitespace
              printable_chars = string.printable
              cleaned_content = ''.join(char if char in printable_chars else ' ' for char in log_content)
              
              # Get the analysis prompt from environment variable
              analysis_prompt = os.environ.get('ANALYSIS_PROMPT', 'Analyze this log file for anomalies.')
              
              # Create the API request
              request_data = {
                  'messages': [
                      {
                          'role': 'user',
                          'content': analysis_prompt + '\\n\\n' + cleaned_content
                      }
                  ],
                  'model': 'openai/gpt-4o'
              }
              
              # Write the properly formatted JSON request
              with open('/tmp/final_request.json', 'w') as f:
                  json.dump(request_data, f)
              
              print('API request prepared successfully')
          
          except Exception as e:
              print(f'Error preparing request: {e}')
              sys.exit(1)
          "
          
          # Call AI model to analyze logs
          RESPONSE=$(curl -s "https://models.github.ai/inference/chat/completions" \
             -H "Content-Type: application/json" \
             -H "Authorization: Bearer $GITHUB_TOKEN" \
             -d @/tmp/final_request.json)
          
          echo "=== ðŸ“Š COMPREHENSIVE LOG ANALYSIS REPORT ==="
          echo "Generated on: $(date)"
          echo "Log file: ${{ env.LOG_FILE_PATH }}"
          echo "Analysis powered by: GitHub Models (GPT-4o)"
          echo "=================================="
          echo ""
          
          # Capture the analysis results
          ANALYSIS_OUTPUT=$(echo "$RESPONSE" | python3 -c "
          import sys, json
          try:
              data = json.load(sys.stdin)
              if 'choices' in data and len(data['choices']) > 0:
                  print(data['choices'][0]['message']['content'])
              else:
                  print('âŒ Error: Unexpected response format')
          except Exception as e:
              print('âŒ Error processing response:', str(e))
          ")
          
          # Display the results
          echo "$ANALYSIS_OUTPUT"
          
          # Create GitHub Job Summary
          echo "# ðŸ“Š Log Analysis Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Generated:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Repository:** ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
          echo "**Log File:** \`${{ env.LOG_FILE_PATH }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Analysis Model:** GitHub Models (GPT-4o)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ“ˆ Log Statistics" >> $GITHUB_STEP_SUMMARY
          echo "- **ERROR entries:** $(grep -i -c -E '\[error\]|\berror\b' ${{ env.LOG_FILE_PATH }} 2>/dev/null || echo '0')" >> $GITHUB_STEP_SUMMARY
          echo "- **WARN entries:** $(grep -i -c -E '\[warn\]|\bwarn\b' ${{ env.LOG_FILE_PATH }} 2>/dev/null || echo '0')" >> $GITHUB_STEP_SUMMARY
          echo "- **CRITICAL entries:** $(grep -i -c -E '\[crit\]|\[critical\]|\bcrit\b|\bcritical\b' ${{ env.LOG_FILE_PATH }} 2>/dev/null || echo '0')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ” AI Analysis Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add the analysis results to the job summary
          echo "$ANALYSIS_OUTPUT" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*This analysis was automatically generated by GitHub Actions using GitHub Models API*" >> $GITHUB_STEP_SUMMARY